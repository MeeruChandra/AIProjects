{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "29281dc8-f579-4031-b6f1-cafb5dd2ab43",
      "metadata": {
        "id": "29281dc8-f579-4031-b6f1-cafb5dd2ab43",
        "outputId": "3269652e-d352-4966-d47c-879cb09b9dbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "YiadzKaohMvo"
      },
      "id": "YiadzKaohMvo",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global credential.helper store"
      ],
      "metadata": {
        "id": "-nGxGM_0iSv6"
      },
      "id": "-nGxGM_0iSv6",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HF_TOKEN= userdata.get(\"HF_TOKEN\")\n",
        "login(HF_TOKEN, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "DxMARWz3hr0d"
      },
      "id": "DxMARWz3hr0d",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# instruct models\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub"
      ],
      "metadata": {
        "id": "XwjJfaYciNbL"
      },
      "id": "XwjJfaYciNbL",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "    {\"role\": \"user\", \"content\": \"Tell a light-hearted joke for a room of Data Scientists\"}\n",
        "  ]"
      ],
      "metadata": {
        "id": "X0pnhE0xijDc"
      },
      "id": "X0pnhE0xijDc",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config=BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype= torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "Qj3ft5XIixSr"
      },
      "id": "Qj3ft5XIixSr",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer =AutoTokenizer.from_pretrained(LLAMA)\n",
        "tokenizer.pad_token= tokenizer.eos_token\n",
        "inputs =tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "sqInNip2ixJa"
      },
      "id": "sqInNip2ixJa",
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "XVtBr8jTiw81",
        "outputId": "94cecbae-a80b-4973-d364-c65389260b10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "XVtBr8jTiw81",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[128000, 128006,   9125, 128007,    271,  38766,   1303,  33025,   2696,\n",
            "             25,   6790,    220,   2366,     18,    198,  15724,   2696,     25,\n",
            "            220,    966,   3799,    220,   2366,     19,    271,   2675,    527,\n",
            "            264,  11190,  18328, 128009, 128006,    882, 128007,    271,  41551,\n",
            "            264,   3177,  70395,  22380,    369,    264,   3130,    315,   2956,\n",
            "          57116, 128009]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config= quant_config)"
      ],
      "metadata": {
        "id": "sJfi5WOu8QEi"
      },
      "id": "sJfi5WOu8QEi",
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "memory = model.get_memory_footprint()/1e6\n",
        "print(f'Memory footprint:{memory:,.1f} MB')"
      ],
      "metadata": {
        "id": "dKuapYvs9umu",
        "outputId": "782de452-6c03-4dd9-bd8d-7f49851e04ea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "dKuapYvs9umu",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint:1,012.0 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "afgGvCx4_j1A",
        "outputId": "81e77bd8-bc47-44b9-b0b1-efaf29bb2dc9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "afgGvCx4_j1A",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForCausalLM(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 2048)\n",
              "    (layers): ModuleList(\n",
              "      (0-15): 16 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=2048, out_features=512, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=2048, out_features=8192, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=8192, out_features=2048, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "        (post_attention_layernorm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm((2048,), eps=1e-05)\n",
              "    (rotary_emb): LlamaRotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=2048, out_features=128256, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs=model.generate(inputs, max_new_tokens=80)\n",
        "print(tokenizer.decode (outputs[0]))"
      ],
      "metadata": {
        "id": "YdE8nL1H_qIE",
        "outputId": "1a4aa6ec-64ec-4c7b-aabc-a5ae1245cf15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "YdE8nL1H_qIE",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 30 Dec 2024\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here's a joke tailored for a room of Data Scientists:\n",
            "\n",
            "Why did the data scientist break up with his girlfriend?\n",
            "\n",
            "Because he wanted to analyze their relationship, and he found it was just a correlation, not an expectation!\n",
            "\n",
            "(Sorry, I know it's a bit of a nerdy pun, but I hope it brought a smile to their faces!)<|eot_id|>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del inputs , outputs, model\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "yOu1zjI8_qEe"
      },
      "id": "yOu1zjI8_qEe",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate (model, messages):\n",
        "  tokenizer =AutoTokenizer.from_pretrained(LLAMA)\n",
        "  tokenizer.pad_token= tokenizer.eos_token\n",
        "  inputs =tokenizer.apply_chat_template(messages,return_tensors=\"pt\").to(\"cuda\")\n",
        "  streamer = TextStreamer(tokenizer)\n",
        "  model = AutoModelForCausalLM.from_pretrained(LLAMA, device_map=\"auto\", quantization_config= quant_config)\n",
        "  memory = model.get_memory_footprint()/1e6\n",
        "  print(f'Memory footprint:{memory:,.1f} MB')\n",
        "  outputs=model.generate(inputs, max_new_tokens=80, streamer = streamer)\n",
        "  del inputs , outputs, model, streamer\n",
        "  torch.cuda.empty_cache()\n",
        ""
      ],
      "metadata": {
        "id": "pHSJsRk9_qBK"
      },
      "id": "pHSJsRk9_qBK",
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate(PHI3,messages)"
      ],
      "metadata": {
        "id": "JptussAA_p97",
        "outputId": "d0bdfb72-bb87-4111-c14a-a5ced0c3c33d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 613
        }
      },
      "id": "JptussAA_p97",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory footprint:1,012.0 MB\n",
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "\n",
            "Cutting Knowledge Date: December 2023\n",
            "Today Date: 30 Dec 2024\n",
            "\n",
            "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
            "\n",
            "Tell a light-hearted joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "Here's a light-hearted joke for a room of data scientists:\n",
            "\n",
            "Why did the data scientist break up with his girlfriend?\n",
            "\n",
            "Because he realized he had a lot of 'data' to process, but she was just a 'variable' in his life. Now he's just seasoning' his heart!\n",
            "\n",
            "(I hope that brings a smile to their faces!)<|eot_id|>\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnboundLocalError",
          "evalue": "local variable 'inputs' referenced before assignment",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-07f4532f53cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPHI3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-29-6f7051830019>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Memory footprint:{memory:,.1f} MB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mdel\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstreamer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'inputs' referenced before assignment"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# instruct models\n",
        "\n",
        "LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
        "PHI3 = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "GEMMA2 = \"google/gemma-2-2b-it\"\n",
        "QWEN2 = \"Qwen/Qwen2-7B-Instruct\" # exercise for you\n",
        "MIXTRAL = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" # If this doesn't fit it your GPU memory, try others from the hub"
      ],
      "metadata": {
        "id": "O1xFEW1j_p6b"
      },
      "id": "O1xFEW1j_p6b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qsizAWDR_p3J"
      },
      "id": "qsizAWDR_p3J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7toFZwG_pii"
      },
      "id": "C7toFZwG_pii",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "R3m1HwtW8i-Z"
      },
      "id": "R3m1HwtW8i-Z"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "",
      "name": ""
    },
    "language_info": {
      "name": ""
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}